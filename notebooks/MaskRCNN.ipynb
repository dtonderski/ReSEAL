{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "from src.config import default_map_builder_cfg, default_sim_cfg\n",
    "from src.features.mapping import SemanticMap3DBuilder\n",
    "from src.model.perception import map_processing\n",
    "from src.model.perception.labeler import LabelGenerator\n",
    "from src.utils import category_mapping\n",
    "from src.utils.category_mapping import get_instance_index_to_reseal_name_dict\n",
    "from src.utils.misc import get_semantic_map\n",
    "from src.visualisation import instance_map_visualization\n",
    "from src.visualisation.instance_map_visualization import visualize_2d_categorical_instance_map\n",
    "from src.visualisation.semantic_map_visualization import (\n",
    "    visualize_categorical_label_map,\n",
    "    visualize_semantic_map,\n",
    ")\n",
    "from src.model.perception.model_wrapper import ModelWrapper\n",
    "\n",
    "if pathlib.PurePath(os.getcwd()).name == 'notebooks':\n",
    "    print(pathlib.PurePath(os.getcwd()).name)\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY = \"00006-HkseAnWCgqk\"\n",
    "ROOT = f\"./data/interim/trajectories/train/{TRAJECTORY}\"\n",
    "DEPTH_MAP_DIR = f\"./data/interim/trajectories/train/{TRAJECTORY}/D\"\n",
    "RGB_IMAGE_DIR = f\"./data/interim/trajectories/train/{TRAJECTORY}/RGB\"\n",
    "POSITIONS_FILE = f\"./data/interim/trajectories/train/{TRAJECTORY}/positions.npy\"\n",
    "ROTATIONS_FILE = f\"./data/interim/trajectories/train/{TRAJECTORY}/rotations.npy\"\n",
    "SEMANTIC_MAP_DIR = f\"./data/interim/trajectories/train/{TRAJECTORY}/Semantic\"\n",
    "trajectory_name = TRAJECTORY.split(\"-\")[1]\n",
    "SEMANTIC_INFO_FILE = f\"./data/raw/train/scene_datasets/hm3d/train/{TRAJECTORY}/{trajectory_name}.semantic.txt\"\n",
    "\n",
    "sim_cfg = default_sim_cfg()\n",
    "map_builder_cfg = default_map_builder_cfg()\n",
    "map_builder_cfg.NUM_SEMANTIC_CLASSES = 6\n",
    "map_builder_cfg.RESOLUTION = 0.05\n",
    "map_builder_cfg.MAP_SIZE = [25, 1.5, 25]\n",
    "map_builder_cfg.GET_ENTIRE_MAP = True\n",
    "map_builder = SemanticMap3DBuilder(map_builder_cfg, sim_cfg)\n",
    "\n",
    "model_config = CfgNode()\n",
    "model_config.USE_INITIAL_TRANSFORMS = True\n",
    "model_config.SCORE_THRESHOLD = 0.5\n",
    "model_config.MASK_THRESHOLD = 0.5\n",
    "model = ModelWrapper(model_config)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:51<00:00,  7.83it/s]\n"
     ]
    }
   ],
   "source": [
    "rotations = np.load(ROTATIONS_FILE).view(dtype=np.quaternion)\n",
    "positions = np.load(POSITIONS_FILE)\n",
    "scene_index_to_category_index_map = category_mapping.get_scene_index_to_reseal_index_vectorized(SEMANTIC_INFO_FILE)\n",
    "\n",
    "map_builder.clear()\n",
    "\n",
    "def load_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image / 255\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,400)):\n",
    "    depth_map = np.load(f\"{DEPTH_MAP_DIR}/{i}.npy\")\n",
    "    rgb_image = load_image(f\"{RGB_IMAGE_DIR}/{i}.png\")\n",
    "    map = model(rgb_image)\n",
    "    # saved_semantics = np.load(f\"{SEMANTIC_MAP_DIR}/{i}.npy\")\n",
    "    # map = get_semantic_map(saved_semantics, scene_index_to_category_index_map, map_builder_cfg.NUM_SEMANTIC_CLASSES)\n",
    "    pose = (positions[i], rotations[i])\n",
    "    map_builder.update_point_cloud(map, depth_map, pose)\n",
    "    if i % 10 == 1:\n",
    "        map_builder.update_semantic_map()\n",
    "\n",
    "map_builder.update_semantic_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_map = map_builder.semantic_map\n",
    "map_processor_cfg = CfgNode()\n",
    "map_processor_cfg.NO_OBJECT_CONFIDENCE_THRESHOLD = 0.5\n",
    "map_processor_cfg.HOLE_VOXEL_THRESHOLD = 2000\n",
    "map_processor_cfg.OBJECT_VOXEL_THRESHOLD = 200\n",
    "map_processor_cfg.DILATE = True\n",
    "\n",
    "grid_index_of_origin = map_builder.get_grid_index_of_origin()\n",
    "\n",
    "label_generator = LabelGenerator(semantic_map, grid_index_of_origin, map_builder_cfg, map_processor_cfg, sim_cfg.SENSOR_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.MaskRCNNDataset import MaskRCNNDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from src.config import default_maskrcnn_cfg\n",
    "import torchvision\n",
    "maskrcnn_cfg = default_maskrcnn_cfg()\n",
    "transforms = torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT.transforms()\n",
    "mask_dataset = MaskRCNNDataset(ROOT, transforms=transforms, label_generator=label_generator)\n",
    "train_dataloader = DataLoader(mask_dataset, maskrcnn_cfg.BATCH_SIZE, maskrcnn_cfg.SHUFFLE)\n",
    "\n",
    "params =  [p for p in model.maskrcnn.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=maskrcnn_cfg.LEARNING_RATE,\n",
    "\t\t\t\t\t\t\tmomentum=maskrcnn_cfg.OPTIM_MOMENTUM, \n",
    "\t\t\t\t\t\t\tweight_decay=maskrcnn_cfg.OPTIM_WEIGHT_DECAY)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tstep_size=maskrcnn_cfg.OPTIM_STEP_SIZE,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tgamma=maskrcnn_cfg.OPTIM_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/451 [00:02<17:22,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/451 [00:03<12:23,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/451 [00:04<10:28,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/451 [00:05<10:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/451 [00:06<09:18,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6/451 [00:07<08:38,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/451 [00:08<07:49,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/451 [00:09<07:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/451 [00:10<06:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/451 [00:10<05:25,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/451 [00:11<05:16,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 12/451 [00:11<05:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/451 [00:12<05:24,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/451 [00:13<05:59,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/451 [00:14<06:47,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 16/451 [00:16<07:06,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 17/451 [00:17<07:12,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 18/451 [00:18<07:20,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/451 [00:19<06:59,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/451 [00:20<07:07,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 21/451 [00:20<06:54,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 22/451 [00:21<06:53,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 23/451 [00:22<06:33,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 24/451 [00:23<05:59,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 25/451 [00:24<05:52,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 26/451 [00:24<05:25,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 27/451 [00:25<05:44,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 28/451 [00:26<05:39,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 29/451 [00:27<05:48,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 30/451 [00:28<05:36,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 31/451 [00:28<05:41,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "image type\n",
      "<class 'PIL.Image.Image'>\n",
      "type after transform\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 31/451 [00:29<06:42,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown image shape: torch.Size([3, 256, 256]) for torch tensor! Must be B,C,H,W.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \t\u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m---> 10\u001b[0m loss \u001b[39m=\u001b[39m model(model_input\u001b[39m=\u001b[39;49mimage, labels\u001b[39m=\u001b[39;49mtarget)[\u001b[39m'\u001b[39m\u001b[39mloss_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/workspaces/ReSEAL/src/model/perception/model_wrapper.py:161\u001b[0m, in \u001b[0;36mModelWrapper.__call__\u001b[0;34m(self, model_input, labels, label_indices_in_reseal_space)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIn train mode, instance_map must be provided.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m model_input_preprocessed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(model_input)\n\u001b[1;32m    162\u001b[0m model_input_preprocessed \u001b[39m=\u001b[39m model_input_preprocessed\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/ReSEAL/src/model/perception/model_wrapper.py:201\u001b[0m, in \u001b[0;36mModelWrapper._preprocess_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(image\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m \u001b[39mor\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 201\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown image shape: \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for torch tensor! Must be B,C,H,W.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown image type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(image)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown image shape: torch.Size([3, 256, 256]) for torch tensor! Must be B,C,H,W."
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(maskrcnn_cfg.NUM_EPOCHS):\n",
    "\tfor image, target in tqdm(train_dataloader):\n",
    "\t\t\n",
    "\t\tprint(\"image shape after loading\")\n",
    "\t\tprint(image.shape)\n",
    "\t\t#device = 'cuda'\n",
    "\t\t#image, target = image.to(device), target.to(device)\n",
    "\t\tif target['boxes'].shape[1] == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tloss = model(model_input=image, labels=target)['loss_mask']\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tlr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('habitat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334581d5fae70a3394a0b95016f9fd1a46cc109bef43a220fa0ce1a6eb571224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

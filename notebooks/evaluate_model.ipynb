{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.mapping import SemanticMap3DBuilder\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from yacs.config import CfgNode\n",
    "from src.model.perception.perception_pipeline_config import get_perception_cfg\n",
    "from src.model.perception.evaluation import get_ground_truth\n",
    "from src.model.perception.labeler import LabelGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from src.model.perception.model_wrapper import ModelWrapper\n",
    "from PIL import Image\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebooks\n"
     ]
    }
   ],
   "source": [
    "if pathlib.PurePath(os.getcwd()).name == \"notebooks\":\n",
    "    print(pathlib.PurePath(os.getcwd()).name)\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = '00800-TEEsavR23oF'\n",
    "epoch = 0\n",
    "TRAJECTORY_DIR = pathlib.Path('data') / 'interim' / 'trajectories' / '23-06-11,16:09:05' / f'epoch_{epoch}' / scene_id\n",
    "SEMANTIC_INFO_PATH = (pathlib.Path('data') / 'raw' / 'minival' / 'scene_datasets' / 'hm3d' / 'minival' \n",
    "                      / scene_id / f'{scene_id.split(\"-\")[1]}.semantic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics_list = []\n",
    "rgb_list = []\n",
    "truth_dicts = []\n",
    "for i in [142, 143, 144]:\n",
    "    semantics = np.load(TRAJECTORY_DIR / 'Semantic' / f'{i}.npy')\n",
    "    semantics_list.append(semantics)\n",
    "    rgb_list.append(Image.open(TRAJECTORY_DIR / 'RGB' / f'{i}.png'))\n",
    "    truth_dicts.append(get_ground_truth(semantics, SEMANTIC_INFO_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.perception.perception_pipeline_config import get_perception_cfg\n",
    "from src.data.MaskRCNNEvaluationDataset import MaskRCNNEvaluationDataset, eval_collate_fn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "perception_cfg = get_perception_cfg()\n",
    "perception_cfg.DATA_PATHS.TRAJECTORIES_DIR = str(Path(\n",
    "        perception_cfg.DATA_PATHS.INTERIM_DATA_DIR, \"trajectories\", \"23-06-11,16:09:05\"))\n",
    "\n",
    "eval_dataset = MaskRCNNEvaluationDataset(perception_cfg.DATA_PATHS, perception_cfg.DATA_GENERATOR.SPLIT, 0)\n",
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                                batch_size=perception_cfg.TRAINING.BATCH_SIZE,\n",
    "                                shuffle=perception_cfg.TRAINING.SHUFFLE,\n",
    "                                num_workers=perception_cfg.TRAINING.NUM_WORKERS,\n",
    "                                collate_fn=eval_collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = CfgNode()\n",
    "model_config.USE_INITIAL_TRANSFORMS = True\n",
    "model_config.SCORE_THRESHOLD = 0.5\n",
    "model_config.MASK_THRESHOLD = 0.5\n",
    "model_config.TRAINABLE_BACKBONE_LAYERS = 0\n",
    "model = ModelWrapper(model_config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for i, (images, semantics, semantic_info_paths) in enumerate(eval_dataloader):\n",
    "    ground_truths = [get_ground_truth(semantics[i], semantic_info_paths[i]) for i in range(len(images))]\n",
    "    metrics.append(model.get_metrics(images, ground_truths))\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14728284006317457"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(metrics)/len(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskRCNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for plotting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/habitat/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Dataloader and MaskRCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set up!!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from src.config import default_maskrcnn_cfg\n",
    "from src.data.MaskRCNNDataset import MaskRCNNDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "cfg = default_maskrcnn_cfg()\n",
    "name = pathlib.PurePath(os.getcwd()).name\n",
    "if(name == 'notebooks'):\n",
    "    os.chdir('..')\n",
    "rootdir = os.getcwd()\n",
    "rootdir = os.path.join(rootdir, \"data/interim/trajectories/train\")\n",
    "scene_names = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    name = pathlib.PurePath(subdir).name\n",
    "    if(name == 'train'):\n",
    "        scene_names = dirs\n",
    "\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "for name in scene_names:\n",
    "    path = os.path.join(rootdir, name+'/RGB')\n",
    "    train_data = MaskRCNNDataset(path, 388, transform=transforms)\n",
    "    train_loader = DataLoader(train_data, cfg.BATCHSIZE, cfg.SHUFFLE)\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device=device)\n",
    "model.eval()\n",
    "print('All set up!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate semantic images list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/torchvision/utils.py:314: UserWarning: masks doesn't contain any mask. No mask was drawn\n",
      "  warnings.warn(\"masks doesn't contain any mask. No mask was drawn\")\n"
     ]
    }
   ],
   "source": [
    "score_threshold = 0.5\n",
    "proba_threshold = 0.5\n",
    "semantic_images_list = []\n",
    "for _, batch in enumerate(train_loader):\n",
    "    batch = batch.to(device=device)\n",
    "    outputs = model(transforms(batch))\n",
    "    batch = batch.to(device='cpu')\n",
    "    bool_masks = [\n",
    "        out['masks'][out['scores']>score_threshold] > proba_threshold\n",
    "        for out in outputs\n",
    "    ]\n",
    "    img_with_masks = [\n",
    "        draw_segmentation_masks(img, mask.squeeze(1))\n",
    "        for img, mask in zip(batch, bool_masks)\n",
    "    ]\n",
    "    semantic_images_list.extend(img_with_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0122, 0.0190, 0.0240,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0173, 0.0270, 0.0342,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0220, 0.0343, 0.0435,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0149, 0.0250, 0.0351,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0186, 0.0312, 0.0439,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0222, 0.0374, 0.0526,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0318, 0.0535, 0.0753,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0162, 0.0272, 0.0382,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0005, 0.0009, 0.0012,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "labels, masks = outputs[0]['labels'], outputs[0]['masks']\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([28, 82, 85], device='cuda:0')\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "dictionary = {28: 1, 82: 0, 85: 0}\n",
    "print(type(dictionary))\n",
    "print(labels)\n",
    "print(type(dictionary.get(labels[0].item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "seal_labels = labels.cpu().apply_(dictionary.get)\n",
    "print(type(seal_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 256])\n",
      "tensor([[[0.0149, 0.0250, 0.0351,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0186, 0.0312, 0.0439,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0222, 0.0374, 0.0526,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0318, 0.0535, 0.0753,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0162, 0.0272, 0.0382,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0005, 0.0009, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "semantic_map_2d = torch.zeros((masks.shape[2], masks.shape[3], 7))\n",
    "ma = masks.squeeze(1)[seal_labels == 0].detach().cpu()\n",
    "print(ma.shape)\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0190, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0240, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0173, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0270, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0342, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0220, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0343, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0435, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "for category in range(1,7):\n",
    "    if category in seal_labels:\n",
    "        semantic_map_2d[:,:,category] = (masks[seal_labels == category].detach().cpu().max(dim=0)[0])[0]\n",
    "        print(semantic_map_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_map_2d = semantic_map_2d[:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 6])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_map_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 7.1143e-02, 9.5931e+01, 1.1912e+02],\n",
      "        [2.2214e-01, 0.0000e+00, 1.1445e+02, 2.5028e+02],\n",
      "        [1.0774e+02, 4.9580e+00, 1.5373e+02, 3.7637e+01]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([28, 82, 85], device='cuda:0')\n",
      "              maskCatName  maskcat\n",
      "maskInstNum                       \n",
      "chair                  62        0\n",
      "couch                  63        1\n",
      "potted plant           64        2\n",
      "bed                    65        3\n",
      "toilet                 70        4\n",
      "tv                     72        5\n"
     ]
    }
   ],
   "source": [
    "from src.utils.datatypes import SemanticMap2D\n",
    "from src.utils.category_mappings import load_mask_instance_to_maskcat\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "label_mapping = load_mask_instance_to_maskcat(cfg)\n",
    "\n",
    "semantic_masks: SemanticMap2D = np.array((256, 256, len(label_mapping.index)))\n",
    "print(outputs[0]['boxes'])\n",
    "for out in outputs:\n",
    "\tprint(out['labels'])\n",
    "\tprint(label_mapping)\n",
    "\t#labels = out['labels'].cpu().numpy()\n",
    "\tlabels = out['labels']\n",
    "\tfor i in labels:\n",
    "\t\tlabel = 63 #labels[i]\n",
    "\t\tif(label in label_mapping.index):\n",
    "\t\t\tchannel = label_mapping.loc[label]['maskcat']\n",
    "\t\t\tprint(channel)\n",
    "\t\t\tsemantic_masks[:,:,channel] = out['masks']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(weights.meta['categories']))\n",
    "print(MaskRCNN_ResNet50_FPN_Weights.DEFAULT.meta['categories'])\n",
    "mask_categories = MaskRCNN_ResNet50_FPN_Weights.DEFAULT.meta['categories']\n",
    "print(mask_categories[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 0\n",
    "max_frames = len(semantic_images_list)\n",
    "i = 3\n",
    "for img in semantic_images_list:\n",
    "   if(i == 0):\n",
    "      break\n",
    "   i -= 1\n",
    "   show(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(weights.meta['categories']))\n",
    "# print(MaskRCNN_ResNet50_FPN_Weights.DEFAULT.meta['categories'])\n",
    "mask_categories = MaskRCNN_ResNet50_FPN_Weights.DEFAULT.meta['categories']\n",
    "print(mask_categories[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tracked_categories_names = {(65,'bed'), (62,'chair'), (63,'couch'), (64,'potted plant'), (70,'toilet'), (72,'tv')}\n",
    "mask_instance_to_mascat = pd.DataFrame(tracked_categories_names, columns={'maskNum', 'maskcat'}).sort_values('maskcat', ignore_index=True).set_index('maskNum')\n",
    "print(mask_instance_to_mascat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('habitat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334581d5fae70a3394a0b95016f9fd1a46cc109bef43a220fa0ce1a6eb571224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
